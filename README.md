# AIFFEL Online Campus 5th Cohort

This README.md file is shared from shinnew_99(https://github.com/shinnew99/Aiffel_Quest/blob/main/README.md)  
23.06.19 ~ 23.12.06
<br>공휴일: 07.17(제헌절), 09.28~10.03(추석연휴) 

<br> **주의**
<br> - Reference 빠짐
<br> - Fundamental 10, 13만 챙김 
<br>
<br>
|기간|배운단계|주요배운 내용|
|----|--------|--------------|
|06.19~06.23|OnboardingQuest|Github Sync|
|06.26~07.07|Fundamental|AI의 기본적인 개념을 수박 겉핥기식으로 다룸 <br>(활성함수 종류 및 평가지표 등) <br>We covered the basic concepts of AI including activation functions and embedding layers from DeepLearning|
|07.10~07.21|Exploration|DL에 필요한 함수 클론코딩 <br>Clone coded the indispensable functions for DL|
|07.24|MainQuest|MainQuest|
|07.25~07.26|딥러닝 한번에 끝내기|DL_Master|
|07.27~08.10|DeppLearningwithPython_2ndEdition|케창딥 책 훑어봄 <br>https://github.com/shinnew99/DeepLearningwithPython_2ndEdition |
|08.11|MainQuest2|암환자 예측하기|
|08.14~08.25|GoingDeeper~05||
|08.28|뭐했더라..|
|08.29~08.30|DL_Thon|KoreaHateSpeech|
|08.31|DL_Thon|MainQuest03_Presentation&Evaluate|
|09.01~09.08|여름방학|방학일기남김|
|09.11.월~09.20.목|GoingDeeper~09|논문작성가이드까지|
|09.21.목|MainQuest04|논문씀|
|논문읽기|SlowPaper 논문목록 (https://www.notion.so/modulabs/SLOW-PAPER-NLP-adedecbe5b904e6d9ee7c530161960b3)| 1. AttentionIsAllYouNeed <br>2. LoRA <br>3. LLM.int8() <br>4. QLoRA|
|09.22~09.25|SlowPaper01|AttentionIsAllYouNeed(https://docs.google.com/presentation/d/1MqeNU0JrgaY8Als9QuuVYFhvXENMPFtc3FbOT5IpL6g/edit?usp=drive_web&ouid=106009627944923904485)|
|09.26~09.27|SlowPaper02|LoRA(https://docs.google.com/presentation/d/1CqXfSn_tHaPKUpVcuWkhvRCnQ7nT-5DtMo-OtIQqrUg/edit)|
|09.28~10.03|추석연휴|Chuseok_Korean Thanksgiving Holiday|
|10.04~10.05|SlowPaper03|LLM.int8()(https://docs.google.com/presentation/d/1YLOhDiw4ktLE6nK9sKNPesNTHo92wsFDss5dnSY1ILk/edit)|
|10.06~10.09|SlowPaper04|QLoRA(https://docs.google.com/presentation/d/199tFaio2voXvMHDA0TPmK9atrxr5boVSZbEFj5FlHjg/edit?usp=drive_web&ouid=106009627944923904485)|
|10.10~10.15|PreAiffelThon||
|10.16~12.04|AiFFELThon|GiTi-4 (visit the page: https://github.com/shinnew99/GiTi-4)|
|12.05~12.06|OffBoarding|We Say Bye to each other|

<br><br>
### Fundamental QUEST
|No.|Project|Task|
|---|-------|----|
|01|파이썬으로 코딩 시작하기 <br>Getting Started with Python Coding|NotYetUploaded|
|02|파이썬 잘하는 척 해보자 <br>Pretending to Be Good at Python|NotYetUploaded|
|03|당신의 행운의 숫자는? 나만의 n면체 주사위 위젯 만들기 <br>What's Your Lucky Number? Creating Your Own n-sided Dice Widget|NotYetUploaded|
|04|Data 어떻게 표현하면 좋을까? 배열(array)과 표(table) <br>How to Represent Data Well? Arrays and Tables|NotYetUploaded|
|05|데이터를 한눈에! Visualization <br>At a Glance! Data Visualization|NotYetUploaded|
|06|사이킷런으로 구현해 보는 머신러닝 <br>Implementing Machine Learning with Scikit-Learn|NotYetUploaded|
|07|다양한 데이터 전처리 기법<br>Various Data Preprocessing Techniques|NotYetUploaded|
|08|가랏, 몬스터볼! 전설의 포켓몬 찾아 삼만리 <br>Go, Monster Ball! Finding Legendary Pokémon Across Thirty Thousand Miles|NotYetUploaded|
|09|Evaluation Metric|NotYetUploaded|
|10|딥리넝 레이어의 이해(1)Linear, Convolution <br>Understanding Deep Learning Layers (1) - Linear, Convolution|NotYetUploaded|
|11|딥리넝 레이어의 이해(2)Embedding, Recurrent <br>Understanding Deep Learning Layers (2) - Embedding, Recurrent|NotYetUploaded|
|12|딥러닝 돌여다보기 <br>Exploring Deep Learning|NotYetUploaded|
|13|딥네트워크, 서로 뭐가 다른 거죠? <br>Deep Networks, What Makes Them Different?|NotYetUploaded|
|14|활성화 함수의 이해 <br>Understanding Activation Functions|NotYetUploaded|
|15|TF2 API 개요 <br>Overview of TensorFlow 2 API|NotYetUploaded|
|16|Regularziation|NotYetUploaded|
|17|선형 회귀와 로지스틱 회귀 <br>Linear Regression and Logistic Regression|NotYetUploaded|
|18|확률 및 확률분포 <br>Probability and Probability Distribution|NotYetUploaded|
|19|Likelihood(MLE와 MAP)|NotYetUploaded|
|20|정보이론 톺아보기 <br>Exploring Information Theory|NotYetUploaded|
|21|비지도학습 <br>Unsupervised Learning|NotYetUploaded|
|22|사이킷런을 활용한 추천 시스템 입문 <br>Introduction to Recommender Systems Using Scikit-Learn|NotYetUploaded|
|23|논문 작성 가이드 <br> A guide to Thesis Writing|NotYetUploaded|


<br><br>
### Eploration QUEST
|No.|Project|Task|
|---|-------|----|
|1|날씨 좋은 월요일 오후 세 시, 자전거 타는 사람은 몇 명? <br>On a sunny Monday afternoon at 3 PM, how many people are riding bicycles?|Linear Regression|
|2|나의 첫 번째 캐글 경진대회, 무작정 따라해보기 <br>My First Kaggle Competition, Trying Blindly|Kaggle ML month with KaKR|
|3|카메라 스티커앱 만들기 첫걸음 <br>First Steps in Creating a Camera Sticker App|Face Detection|
|4|영화리뷰 텍스트 감성분석하기 <br>Sentiment Analysis of Movie Review Texts|Text Classification|
|5|인물사진을 만들어 보자 <br>Let's Create Portrait Photos|Semantic Segmentation|
|6|뉴스 요약봇 만들기 <br>Creating a News Summary Bot|Text Summarization|
|7|난 스케치를 할 테니 너는 채색을 하거라 <br>I'll Sketch, You Do the Coloring|Conditional GAN|
|8|트랜스포머로 만드는 대화형 챗봇 <br>Building an Interactive Chatbot with Transformer|Transformer|


<br><br>
### DL_Master
<br> <b>딥러닝 한번에 끝내기 모듈</b>
|No.|Contents|Task|
|---|--------|----|
|1| 딥러닝이란? <br>What is Deep Learning? | 인공신경망의 개념, 인공신경망과 딥러닝의 역사를 통해 딥러닝이 무엇인지 알 수 있습니다. <br>Learn about the concept of artificial neural networks and understand what deep learning is through the history of artificial neural networks and deep learning |
|2| 텐서 표현과 연산 <br>Tensor Representation and Operations | 텐서란 무엇일까요? 이 노드에서는 텐서 개념, 텐서 데이터 타입을 이해하고, 텐서 연산을 수행해 봅니다. <br>What is a tensor? In this node, understand the concept of tensors, tensor data types, and perform tensor operations |
|3| 딥러닝 구조와 모델 <br>Deep Learning Structures and Models | 딥러닝 모델(네트워크)를 구성하는 레이어에 대한 개념을 이해하고, 딥러닝 모델을 구성하는 방법에 대해서 학습합니다. <br>Understand the concepts of layers that make up deep learning models (networks) and learn how to structure deep learning models|
|4| 딥러닝 모델 학습 <br>Deep Learning Model Training | 딥러닝 모델을 학습하기 위한 개념을 이해하고, 다양한 손실 함수, 옵티마이저, 지표에 대해서 학습합니다. <br>Understand the concepts for training deep learning models, and learn about various loss functions, optimizers, and metrics|
|5| 모델 저장과 콜백 <br>Model Saving and Callbacks | 딥러닝 모델을 저장하고 복원하는 방법과 모델 학습 시에 사용할 수 있는 다양한 콜백 함수에 대해 학습합니다. <br>Learn how to save and restore deep learning models and explore various callback functions that can be used during model training.|
|6| 모델 학습 기술 <br>Model Training Techniques | 딥러닝 모델 학습을 위한 다양한 개념과 기술들, 그리고 모델 학습이 잘 안될 경우 발생하는 과소적합/과대적합에 대해 학습합니다. 또한 IMDB 데이터셋을 이용해 긍정/부정 분류를 위한 딥러닝 모델을 만들어봅니다. <br>Learn various concepts and techniques for training deep learning models. Also, understand underfitting and overfitting, and create a deep learning model for positive/negative classification using the IMDB dataset|
|7| 모델 크기 조절과 규제 <br>Model Size Adjustment and Regularization | 딥러닝 모델의 크기 조절 방법과 효과적인 딥러닝 모델 학습을 위한 다양한 규제 방법(L1, L2, L1+L2)을 학습합니다. <br>Learn methods for adjusting the size of deep learning models and various regularization techniques (L1, L2, L1+L2) for effective model training|
|8| 가중치 초기화와 배치정규화 <br>Weight Initialization and Batch Normalization | 딥러닝 모델의 효과적인 학습을 위한 가중치 초기화와 배치 정규화를 알아봅니다. Reuters 데이터셋을 이용해 가중치 초기화와 배치 정규화를 실습합니다. <br>Explore weight initialization and batch normalization for effective training of deep learning models. Practice weight initialization and batch normalization using the Reuters dataset|
|9|딥러닝 모델 실습 <br>Deep Learning Model Practical Exercise | 그동안 배웠던 내용을 새로운 데이터셋인 Fashion MNIST에 적용해 봅니다. 특히 빠른 학습과 과대 적합을 방지하는 모델 최적화를 다양하게 실습합니다. <br>Apply the concepts learned so far to a new dataset, Fashion MNIST. Practice various model optimizations, focusing on fast training and preventing overfitting|
|10|딥러닝 프로젝트 3개 <br>Three Deep Learning Projects | 다음표에 있음 <br>Listed in the table below|

<br> <b>3 Projects</b>
|Proejct|Contents|
|-------|--------|
|Project 1|Boston 주택 가격 예측 프로젝트 <br>Boston Housing Prediction Project|
|Project 2|Reuters 데이터 분류 프로젝트 <br>Classifying TextData from Reuters|
|Project 3|CIFAR10 데이터 분류 프로젝트 <br>Classifying CIFAR 10 Data|


<br><br>
### GoingDeeper NLP
|GD#|Node Number|Title|Contents|
|---|-----------|-----|--------|
|GD01|1|텍스트 데이터 다루기 <br>Handling Text Data | 다양한 텍스트 데이터 전처리 기법을 소개, Word나 형태소 레벨의 tokenizer 및 subword 레벨 tokenizing 기법(BPE, sentencepiece) 학습 <br>Introducing various text data preprocessing techniques, including tokenizers at the word or morpheme level, and subword-level tokenizing techniques (BPE, sentencepiece)|
|GD01|2&3|멋진 단어사전 만들기 <br>Creating an Impressive Vocabulary | [PROJECT] 단어사전을 만들어보고 이를 토대로 perplexity를 측정해보는 프로젝트 <br>Create a vocabulary and measure perplexity based on it as a project|
|GD02|4|텍스트의 분포로 벡터화 하기 <br>Vectorizing Text Based on Distribution | 텍스트 분포를 이용한 텍스트의 벡터화 방법들(BoW, DTM, TF-IDF, LSA, LDA) <br>Methods for vectorizing text using text distributions (BoW, DTM, TF-IDF, LSA, LDA)|
|GD02|5&6|뉴스 카테고리 다중분류 <br>News Category Multi-Classification | [PROJECT] 뉴스 텍스트의 주제를 분류하는 task를 다양한 기법으로 시도해보고 비교, 분석 하는 프로젝트 <br>[PROJECT] Attempting and comparing various techniques for classifying the topics of news text, with analysis, as a project|
|GD03|7|워드 임베딩 <br>Word Embeddings | 워드 임베딩 벡터(Word2Vec, FastText, Glove)의 원리와 사용법을 학습 <br>Learning the principles and usage of word embedding vectors (Word2Vec, FastText, Glove)|
|GD03|8&9|WEAT | [PROJECT] WEAT(Word Embedding Association Test) 기법으로, Word Embedding Model 의 편향성 측정 <br>Measuring the bias of Word Embedding Models using the WEAT (Word Embedding Association Test) technique|
|GD04|10|Seq2seq와 Attention <br>Seq2seq and Attention | 언어 모델이 발전해 온 과정에 대해 배우고, Seq2seq에 대해 학습 <br>Learning about the evolution of language models and understanding Seq2seq|
|GD04|11&12|Seq2seq으로 번역기 만들기 <br>Creating a Translator with Seq2seq | Attention 기법을 추가하여 Seq2seq 기반의 번역기 성능을 높여보기 <br>Improving the performance of a Seq2seq-based translator by adding Attention|
|GD05|13|Transformer가 나오기까지 <br>Until the Emergence of Transformers | Attention 복습 및 트랜스포머에 포함된 모듈을 심층적으로 이해하는 단계 <br>Reviewing Attention and deeply understanding the modules included in Transformers|
|GD05|14&15|Transformer로 번역기 만들기 <br>Creating a Translator with Transformers | 트랜스포머를 이용해 번역기를 만드는 프로젝트 <br>Creating a translator using Transformers as a project|
|GD06|16|기계 번역이 걸어온 길 <br>The Journey of Machine Translation | 자연어 처리에서 Data Augmentation은 어떻게 하는지, 자연어 처리 성능은 어떻게 측정할 수 있는지 학습 <br>Learning how Data Augmentation is done in natural language processing and how the performance of natural language processing is measured|
|GD06|18&19|번역가는 대화에도 능하다 <br>Translators Are Proficient in Conversations Too | 다양한 디코딩 방식을 활용해 모델 구현 후 BLEU Score를 이용하여 성능 평가, 한국어 챗봇 구현 프로젝트 수행 <br>Implementing a model using various decoding methods, evaluating performance using BLEU Score, and performing a Korean chatbot implementation project|
|GD07|20|modern NLP의 흐름에 올라타보자 <br>Riding the Flow of Modern NLP | 트랜스포머를 바탕으로 한 최근 NLP 모델에 대해 학습 <br>Learning about recent NLP models based on Transformers|
|GD07|21|BERT pretrained model 제작 <br>Creating a BERT Pretrained Model | 대표적인 pretrained language model인 BERT 원리에 대해 학습 <br>Learning about the principles of BERT, a representative pretrained language model|
|GD08|22|NLP Framework의 활용 <br>Utilizing NLP Frameworks | 최다양한 NLP Framework에 대해 학습하고, Huggingface transformer를 중심으로 설계구조와 활용법 학습 <br>Learning about various NLP frameworks, focusing on the design structure and usage of Hugging Face Transformer|
|GD08|23&24|16 <b>여기빈칸채우기</b>|HuggingFace 커스텀 프로젝트 만들기 <br>Creating a Custom Project with Hugging Face | Huggingface transformer를 활용한 커스텀 프로젝트 수행 <br>Performing a custom project using Hugging Face Transformer|
|GD09|25|#NLP Trend Note 1 | 최신 LLM 소개, InstructGPT의 SFT, RM, PPO 학습 메커니즘 소개 <br>ntroduction to the latest LLM, introduction to the training mechanisms of InstructGPT's SFT, RM, PPO|
|GD09|26&27|#NLP Trend Note 2 | KochatGPT 구현 <br>Implementation of KochatGPT|


<br><br>
### GoingDeeper CV
|GD#|Node Number|Title|Contents|
|---|-----------|-----|--------|
|GD01|1|백본 네트워크 구조 상세분석 <br>Backbone Network Structure Detailed Analysis | 수정예정<br>영어번역예정|
|GD01|2&3|없다면 어떻게 될까?(ResNet Ablation Study) <br>What If It Doesn't Exist? (ResNet Ablation Study) | [PROJECT]ResNet Ablation Study|
|GD02|4|잘 만든 Augmentation, 이미지 100장 안부럽다수정예정 <br>Well-Crafted Augmentation, Not Inferior to 100 Images  | 수정예정<br>영어번역예정|
|GD02|5&6|이미지 어디까지 우려볼까? <br>How Far Can We Enhance Images? | [PROJECT]Data Augmentation|
|GD03|7|너의 속이 궁금해 - CLass Activation Map <br>Curious About Your Insides - Class Activation Map | 수정예정 <br>영어번역예정|
|GD03|8&9|나를 찾아줘 - Class Activation Map 만들기 <br>Find Me - Creating a Class Activation Map| 수정예정<br>영어번역예정|
|GD04|10|Object Detection | 수정예정 <br>영어번역예정|
|GD04|11&12|GO/STOP! - Object Detection 시스템 만들기 <br>GO/STOP! - Creating an Object Detection System | Class Activation Map|
|GD05|13|물체를 분리하자! - 세그멘테이션 살펴보기 <br>Let's Separate Objects! - Exploring Segmentation | 수정예정<br>영어번역예정|
|GD05|14&15|도로 영역을 찾자! - 세그멘테이션 모델 만들기 <br>Find the Road Area! - Creating a Segmentation Model | 수정예정<br>영어번역예정|
|GD06|16|OCR 기술의 개요 <br>Overview of OCR Technology | 수정예정<br>영어번역예정|
|GD06|17&18|직접 만들어보는 OCR_Optical Character Recognition <br>Creating OCR - Optical Character Recognition |OCR_Optical Character Recognition|
|GD07|19|멀리 있지만 괜찮아 <br>It's Okay Even If It's Far Away | 수정예정<br>영어번역예정|
|GD07|20&21|멀리 있는 사람도 스티커를 붙여주자 <br>Let's Stick Stickers on Distant People | One-Stage Object Detection|
|GD08|22|사람의 몸짓을 읽어보자 <br>Let's Read Human Gestures | 수정예정<br>영어번역예정|
|GD08|23&24|행동 스티커 만들기 <br>Creating Action Stickers | OCR_Optical Character RecognitionHuman Pose Estimation|
|GD09|25|CV 최신 트렌드 맛보지 않으시겠습니까 <br>Why Not Taste the Latest Trends in Computer Vision? | 수정예정<br>영어번역예정|
|GD09|26&27|Stable Diffusion, 너 Do? 나 Do! <br>|Stable Diffusion, What Do You Do? What Do I Do! | Stable Diffusion|


<br><br>
### Main Quest
|No|Project|Contents|
|--|-------|--------|
|1|폐렴아 기다려라!|폐사진 보고 환자/비환자 예측하기 <br>Diagnosing Pnuemonia based on Image|
|2|PASCAL VOC 2012|Image Augmentation|
|3|Motorcycle Night Road|<b>수정예정</b>|
|4|논문쓰기|SentimentAnalysis관련해서 간단하게 논문씀|

